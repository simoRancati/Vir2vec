{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3955fc23-36e3-4eb3-b387-4b183effc4d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Mistral-DNA'...\n",
      "remote: Enumerating objects: 331, done.\u001B[K\n",
      "remote: Counting objects: 100% (96/96), done.\u001B[K\n",
      "remote: Compressing objects: 100% (90/90), done.\u001B[K\n",
      "remote: Total 331 (delta 27), reused 30 (delta 5), pack-reused 235 (from 1)\u001B[K\n",
      "Receiving objects: 100% (331/331), 112.69 MiB | 22.64 MiB/s, done.\n",
      "Resolving deltas: 100% (100/100), done.\n"
     ]
    }
   ],
   "source": [
    "# DOWNLOAD Mistral-DNA GIT REPO\n",
    "!git clone https://github.com/raphaelmourad/Mistral-DNA.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cea80d9-1243-4640-921e-fc93e0f2c410",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (4.51.3)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m40.9/40.9 kB\u001B[0m \u001B[31m3.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.local/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.local/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.10/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.local/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\n",
      "Downloading transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.8/10.8 MB\u001B[0m \u001B[31m34.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m471.6/471.6 kB\u001B[0m \u001B[31m121.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: safetensors, transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.51.3\n",
      "    Uninstalling transformers-4.51.3:\n",
      "      Successfully uninstalled transformers-4.51.3\n",
      "\u001B[33m  WARNING: The scripts transformers and transformers-cli are installed in '/home/simonerancati/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001B[0m\u001B[33m\n",
      "\u001B[0mSuccessfully installed safetensors-0.5.3 transformers-4.53.2\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.3.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython -m pip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bddb2f8a-0b47-41fc-98b1-8ac9232d62de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments\n",
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03e853c4-1dc1-4c57-b681-f12712ed277f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul 16 03:27:27 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA L4                      On  |   00000000:81:00.0 Off |                    0 |\n",
      "| N/A   19C    P8             15W /   70W |       0MiB /  23034MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# CHECK GPU\n",
    "# We can see how many VRAM is used and how much the GPU is used.\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd3c7945-7639-453c-8728-0946d48f97c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.26.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tested with numpy==1.26.4\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65b5caec-c23f-4f36-80f3-07d73e6a6c78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.53.2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tested with transformers==4.47.1\n",
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f46321b9-3f09-46bc-ae18-2c220c177451",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tested with accelerate==0.32.1\n",
    "import accelerate\n",
    "accelerate.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "217ce747-e1aa-4386-bf29-c2b1961facad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark=True\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52056a04-d6c9-4274-a006-1edd4bc2fe82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core.python-7-1752650750-3107-3103-c0605a-s12.ufhpc.1640071\n",
      "Desktop\n",
      "do_Llama3_v2.ipynb\n",
      "embeddings_results_mistral_covid\n",
      "Mistral-DNA\n",
      "ondemand\n",
      "Pat10_72_TM_7188716.log\n",
      "Pat11_72_TM_7188717.log\n",
      "Pat13_36_TM_7188734.log\n",
      "Pat13_72_TM_7188719.log\n",
      "Pat9_72_TM_7188715.log\n",
      "tutorial-env\n",
      "Untitled.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ba6c0f0-64cd-46d1-b0ec-930941d1a631",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/simonerancati/Mistral-DNA\n"
     ]
    }
   ],
   "source": [
    "# SET DIRECTORY\n",
    "os.chdir(\"Mistral-DNA/\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07f3cb53-884c-4c89-81c2-7ab20fa387dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixtralForCausalLM(\n",
       "  (model): MixtralModel(\n",
       "    (embed_tokens): Embedding(4096, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x MixtralDecoderLayer(\n",
       "        (self_attn): MixtralAttention(\n",
       "          (q_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (k_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "        )\n",
       "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "          (gate): Linear(in_features=256, out_features=64, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-63): 64 x MixtralBlockSparseTop2MLP(\n",
       "              (w1): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (w2): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (w3): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): MixtralRMSNorm((256,), eps=1e-05)\n",
       "        (post_attention_layernorm): MixtralRMSNorm((256,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MixtralRMSNorm((256,), eps=1e-05)\n",
       "    (rotary_emb): MixtralRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=256, out_features=4096, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHOOSE THE LLM ARCHITECTURE\n",
    "# To do during class:\n",
    "# - look at the original archicture of Mixtral-8x7B-v0.1, discuss the model\n",
    "# - change the model architecture by adding or removing transformer blocks, hidden states, number of attention heads, and number of experts\n",
    "# - test BERT model architecture?\n",
    "# NB: flash attention 2 does not work with T4 GPU\n",
    "config = AutoConfig.from_pretrained(\"data/models/Mixtral-8x7B-v0.1\", cache_dir = \"/blue/salemi/share/varcovid\") # Mixture of expert\n",
    "#model = AutoModelForCausalLM.from_config(config,attn_implementation=\"flash_attention_2\")\n",
    "model = AutoModelForCausalLM.from_config(config,attn_implementation=\"eager\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d69d9b09-2e08-49af-bbcd-b448b3149485",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab90440096a4c0b99f447acf3b6704f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/158 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4c8b1f56da4a37875780311c88168f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='zhihan1996/DNABERT-2-117M', vocab_size=4096, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# LOAD BPE LETTER TOKENIZER\n",
    "# Question during class: what are UNK, CLS, SEP, PAD and MASK?\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True, cache_dir = \"/blue/salemi/share/varcovid\")\n",
    "tokenizer.padding_side  = 'left'\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e985725d-7f87-4bc2-a6ee-76850e91ee26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   3,    3,    3,    3,    3,    1, 2061,  281,  485,   35,  245, 2179,\n",
      "          102,   13,    2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# DNA ENCODING\n",
    "# During class change the DNA sequence and see what happens.\n",
    "# Change max_length, padding, truncation, ...\n",
    "#encoding = tokenizer(\"ATTT\", padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "encoding = tokenizer(\"ATTGTGGGTCCCCGTAGATGATAGGGGCCCCCC\", max_length=15, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19bb50d5-2f3f-4d80-88f3-76eca899b552",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 105.0M parameters\n"
     ]
    }
   ],
   "source": [
    "# NUMBER OF MODEL PARAMETERS\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model size: {pytorch_total_params/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3550cabe-458f-42c9-854f-c564b9beaad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8d834bb-b9d6-4f9a-a8a0-4bab2dce21af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8422bcd632c14c86afbe5e08f2c1ef51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['TAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCCTAACCCTAACCCTAACCCTAACCCTAACCTAACCCTAACCCTAACCCTAA',\n",
       " 'CCCTAACCCTAACCCTAACCCTAACCCTAACCCCTAACCCTAACCCTAAACCCTAAACCCTAACCCTAACCCTAACCCTAACCCTAACCCCAACCCCAACCCCAACCCCAACCCCAACCCCAACCCTAACCCCTAACCCTAACCCTAACCCTACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCCTAACCCC',\n",
       " 'TAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCCTAACCCTAACCCTAACCCTAACCCTCGCGGTACCCTCAGCCGGCCCGCCCGCCCGGGTCTGACCTGAGGAGAACTGTGCTCCGCCTTCAGAGTACCACCGAAATCTGTGCAGAGGACAACGCAGCTCCGCCCTCGCGGTGCTCTCCGGGTCTGTGCT',\n",
       " 'GAGGAGAACGCAACTCCGCCGTTGCAAAGGCGCGCCGCGCCGGCGCAGGCGCAGAGAGGCGCGCCGCGCCGGCGCAGGCGCAGAGAGGCGCGCCGCGCCGGCGCAGGCGCAGAGAGGCGCGCCGCGCCGGCGCAGGCGCAGAGAGGCGCGCCGCGCCGGCGCAGGCGCAGAGAGGCGCGCCGCGCCGGCGCAGGCGCAGA',\n",
       " 'CACATGCTAGCGCGTCGGGGTGGAGGCGTGGCGCAGGCGCAGAGAGGCGCGCCGCGCCGGCGCAGGCGCAGAGACACATGCTACCGCGTCCAGGGGTGGAGGCGTGGCGCAGGCGCAGAGAGGCGCACCGCGCCGGCGCAGGCGCAGAGACACATGCTAGCGCGTCCAGGGGTGGAGGCGTGGCGCAGGCGCAGAGACGC']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_text = load_dataset(\"csv\", data_files=\"data/genome_sequences/hg38/sequences_hg38_200b_verysmall.csv.gz\")\n",
    "dataset_text['train']['text'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7641692f-804a-475d-a948-d854efe7025d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ef9058724a41eea89e79d43a85fdfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 99999\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# TOKENIZE DATA\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "dataset = dataset_text.map(tokenize_function, batched=True)\n",
    "print(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05869e89-aaab-427a-aa5d-66ccc7199d1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset[\"train\"]))\n",
    "test_size = len(dataset[\"train\"]) - train_size\n",
    "train_set, val_set = torch.utils.data.random_split(dataset[\"train\"], [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8ed7774-75a5-4292-aea2-c7787978f285",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'CGCCAGCCTCTGCACCCCTACTGCTGGGCCCCATGGGAGGCGGGCGCCCGGGTGCTCAGAGACCGCGCCCTCCCCGTCAGGCAGACGCTGCTCTGCCTTCCGGTTCCCTTATGGGGCCAGAGTCTATACAAATGAGCACACAGCTACACTGAGGTACACACAACCACAGCCGGAGACCATCCACAGAGCTGGGCCCCACT',\n",
       " 'input_ids': [3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  504,\n",
       "  1411,\n",
       "  1472,\n",
       "  236,\n",
       "  28,\n",
       "  136,\n",
       "  249,\n",
       "  165,\n",
       "  72,\n",
       "  601,\n",
       "  72,\n",
       "  1994,\n",
       "  50,\n",
       "  13,\n",
       "  601,\n",
       "  78,\n",
       "  2796,\n",
       "  840,\n",
       "  403,\n",
       "  178,\n",
       "  64,\n",
       "  3160,\n",
       "  1128,\n",
       "  97,\n",
       "  560,\n",
       "  2666,\n",
       "  49,\n",
       "  693,\n",
       "  44,\n",
       "  786,\n",
       "  59,\n",
       "  138,\n",
       "  44,\n",
       "  862,\n",
       "  231,\n",
       "  33,\n",
       "  262,\n",
       "  401,\n",
       "  728,\n",
       "  136,\n",
       "  34,\n",
       "  1038,\n",
       "  2],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXAMINE THE DATA\n",
    "# What is input ids?\n",
    "# What is attention mask?\n",
    "train_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b69d513-5af2-4681-bbbc-0c05a2c12d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79999"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d33b8db-b50a-4631-bf7b-e30e022ff28b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 504,\n",
       " 1411,\n",
       " 1472,\n",
       " 236,\n",
       " 28,\n",
       " 136,\n",
       " 249,\n",
       " 165,\n",
       " 72,\n",
       " 601,\n",
       " 72,\n",
       " 1994,\n",
       " 50,\n",
       " 13,\n",
       " 601,\n",
       " 78,\n",
       " 2796,\n",
       " 840,\n",
       " 403,\n",
       " 178,\n",
       " 64,\n",
       " 3160,\n",
       " 1128,\n",
       " 97,\n",
       " 560,\n",
       " 2666,\n",
       " 49,\n",
       " 693,\n",
       " 44,\n",
       " 786,\n",
       " 59,\n",
       " 138,\n",
       " 44,\n",
       " 862,\n",
       " 231,\n",
       " 33,\n",
       " 262,\n",
       " 401,\n",
       " 728,\n",
       " 136,\n",
       " 34,\n",
       " 1038,\n",
       " 2]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[1][\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f22e23e1-90c8-4b05-afcb-92abf92e8da4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=epoch,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=50,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_revision=None,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0005,\n",
      "length_column_name=length,\n",
      "liger_kernel_config=None,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=50,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./results/models,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./results/models,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# PARAMETERS FOR PRETRAINING\n",
    "# During class, modify:\n",
    "# - batchsize\n",
    "# - number of epochs\n",
    "# - learning rate\n",
    "# - weight_decay\n",
    "# - gradient accumulation steps\n",
    "\n",
    "batchsize=32\n",
    "training_args = TrainingArguments(\n",
    "        output_dir='./results/models',\n",
    "        eval_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        num_train_epochs=50,\n",
    "        per_device_train_batch_size=batchsize,\n",
    "        per_device_eval_batch_size=batchsize,\n",
    "        learning_rate=5e-4,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        load_best_model_at_end=True,\n",
    "        bf16=True,\n",
    "        gradient_accumulation_steps=50,\n",
    "        report_to=\"none\",   # <‑‑ disattiva callbacks verso W&B\n",
    ")\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739b0043-42e1-4ab8-aa27-6597283ed31f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wandb in /home/simonerancati/.local/lib/python3.10/site-packages (0.19.11)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/simonerancati/.local/lib/python3.10/site-packages (from wandb) (8.2.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/simonerancati/.local/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/simonerancati/.local/lib/python3.10/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /apps/llama/3/lib/python3.10/site-packages (from wandb) (4.2.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /home/simonerancati/.local/lib/python3.10/site-packages (from wandb) (6.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/simonerancati/.local/lib/python3.10/site-packages (from wandb) (7.0.0)\n",
      "Requirement already satisfied: pydantic<3 in /home/simonerancati/.local/lib/python3.10/site-packages (from wandb) (2.11.5)\n",
      "Requirement already satisfied: pyyaml in /apps/llama/3/lib/python3.10/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/simonerancati/.local/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/simonerancati/.local/lib/python3.10/site-packages (from wandb) (2.29.1)\n",
      "Requirement already satisfied: setproctitle in /home/simonerancati/.local/lib/python3.10/site-packages (from wandb) (1.3.6)\n",
      "Requirement already satisfied: setuptools in /apps/llama/3/lib/python3.10/site-packages (from wandb) (69.5.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /home/simonerancati/.local/lib/python3.10/site-packages (from wandb) (4.14.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /apps/llama/3/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/simonerancati/.local/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/simonerancati/.local/lib/python3.10/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/simonerancati/.local/lib/python3.10/site-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/simonerancati/.local/lib/python3.10/site-packages (from pydantic<3->wandb) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/simonerancati/.local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/simonerancati/.local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/simonerancati/.local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/simonerancati/.local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/simonerancati/.local/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Start a trainer...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 471/2500 54:39 < 3:56:26, 0.14 it/s, Epoch 9.40/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.708786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.586504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.451020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.296253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.157484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.061378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.013256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.980645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.994769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PRETRAIN MODEL\n",
    "# During class, look at:\n",
    "# - the use of CPU RAM, GPU RAM\n",
    "# - estimates how long it takes for 1 epoch\n",
    "\n",
    "# Don't train until the end as it takes too much time (20h).\n",
    "\n",
    "# with 105Mb Mixtral model, 110h / 50 epochs (1M sequences) = 2h / epoch\n",
    "\n",
    "!pip install wandb\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "print ('Start a trainer...')\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5819be2-94f4-4175-a11a-54e7dbc4a580",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Llama3",
   "language": "python",
   "name": "llama3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
